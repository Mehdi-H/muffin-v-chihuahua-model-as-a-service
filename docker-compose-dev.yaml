version: '3.8'

services:

  display-predictions-with-model-as-a-service:
    image: muffin-v-chihuahua-frontend:v1
    build:
      context: frontend
    environment:
      - INFERENCE_HOST=ml-web-service
    ports:
      - 8090:8090

  ml-web-service:
    image: muffin-v-chihuahua-backend:v1
    build:
      context: backend
    ports:
      - 8000:8000
